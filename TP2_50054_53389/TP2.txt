Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1: O mapa usado na maior parte dos testes deste exercício tem as dimensões de 14x14 com borda de 1 produzindo imagens de dimensões 16x16. Tendo estas dimensões como input, começamos a processar as imagens usando intervaladamente 3 camadas de convolução (com 16, 32 e 64 filtros respetivamente) e 2 de MaxPooling para reduzir as dimensões dos outputs. A função de ativação escolhida para as camadas de convolução foi a função ReLu pois reduz a probabilidade de ocorrências de vanishing gradient e o número de ativações de neurónios na rede. A função escolhida para inicializar o kernel das camadas referidas foi a função HeUniform.
De seguida, é usada uma camada Flatten, para reduzir as dimensões do output da camada anterior, e uma camada densa de 64 que recorre também à função de ativação ReLu. Por fim, uma camada densa com o tamanho do espaço de ações (3 – correspondendo às possíveis ações da cobra) e com a função de ativação linear que tenta encontrar os melhores pesos possíveis.
A função de loss escolhida foi a função Huber. Experimentámos também a usar a função de Mean Squared Error (MSE) mas a função Huber que conjuga as funções MSE e MAE revelou-se como a melhor e mais equilibrada alternativa.
Através do tensorboard obtivemos um gráfico dos resultados da função de loss que ajuda a visualizar o impacto da arquitetura escolhida para esta rede.
loss.png


Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2: Inicialmente tentamos otimizar para um cenário 14vs14 para ser pequeno pois, no caso de ser muito grande, ia demorar mais a treinar, e iria ser mais difícil para a cobra comer uma maçã nas primeiras iterações. Além disso usámos borda = 1, sem relva e apenas uma maçã. 
Contudo começámos a detetar um problema no comportamento da cobra. Acabava sempre por se comer a ela mesma, ao virar 3 vezes seguidas para o lado. Às vezes só chegava mesmo a fazer 3 ações (3 para o lado), outras ainda parecia que se iamos ter resultados positivos mas acabava sempre por se comer.
Por isso, decidímos experimentar com relva, já que ao dar reward por comer relva, vai incentivar à cobra a não passar por sítios por onde passou, e assim promover à exploração de novos caminhos. Mas como depois poderia chegar a um ponto em que já não havia sítios com relva, decidimos aplicar também um crescimento da relva de 0.001.
Em suma, o cenário final tem um tabuleiro 14vs14 com uma casa de borda, e com relva, já que foi o caso onde a cobra conseguiu ter mais rewards positivas, o que já era de esperar pois a relva dá reward, porém verificamos que era mesmo o caso onde conseguia comer mais maçãs e obter melhores valores da Huber Loss.


Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3: Para gerar jogos para, mais tarde, treinar o nosso agente, decidimos implementar um algoritmo para simular um jogo humano. No nosso algoritmo a cobra tende a seguir em frente até chegar 
à coluna ou linha (depende da orientação da cobra) da maçã. Após isto, vira para o lado da maçã, e come-a. Sempre que a maçã aparecer nas costas da cobra, ela verifica a posição dela. Se 
a cabeça estiver mais para direita do tabuleiro, e se estiver a descer, vira para a direita (para se afastar das bordas), e depois poder inverter a marcha. A distribuição de ações 
conseguidas é bastante boa, conseguindo um balanço entre as 3 decisões possíveis. 
Os jogos gerados são guardados numa replay_memory com tamanho máximo de 100.000, e são utilizados mais tarde para o modelo aprender, em batchs de 1000 exemplos cada. 
Para alcançar um equilibrio entre exploração aleatório decidimos começar por experimentar a técnica e-greedy exploration que tem uma probabilidade baixa de fazer uma ação random (5% no nosso caso). 
Porém, como não estavamos a conseguir obter bons resultados optámos por experimentar a estratégia Decaying e-greedy exploration que começa com 100% de probabilidade do próximo passo ser random, e ao longo do processo de treino vai diminuindo, começando a tomar mais ações por si e menos random, e conseguimos obter melhores resultados.
Os resultados das ações da cobra vão sendo guardados na replay_memory com o formato, [estado do tabuleiro; ação; reward; estado do tabuleiro após ação; informação se chegou a estado final].
Mais tarde, a cada 20 passos a cobra aprende com a informação da replay_memory e, a cada 200 passos, são ajustados os pesos. Começámos por usar valores mais altos para o caso do treino (treinar a cada 50 passos), mas como estávamos com um problema e a cobra não parecia aprender, decidimos baixar este valor.


Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4: O algoritmo de treino usado foi o Q-Learning, também utilizado nas aulas práticas, que pretende estimar o outcome de uma ação, num determinado estado, parecendo ser o indicado. 
Como decidimos usar relva, que dá uma reward (ainda que pequena), decidímos usar um discount_factor elevado pois assim dará mais importância a rewards futuras (e maiores), como é o caso da maçã, do que rewards imediatas e pequenas, como é o caso da relva, para que, assim, a cobra não fique satisfeita só por comer. 
Contudo, também quisemos ter a certeza de que não usávamos um valor muito alto, para isso testámos com 0.8 e 0.6. (PÔR QUAL FOI O MELHOR).
Após isto, para gerar novas experiências, optámos pela estratégia Decayin e-greedy exploration que começa com uma probabilidade de 100% da cobra aptar por uma ação random, e à medida que o modelo treina, a probabilidade de haver uma ação random vai diminuindo com um decay=0.004. A ideia desta estratégia é a cobra aprender logo no início, e ao longo do processo de treino, vai dependendo menos de ações random, e começa a tomar decisões por ela. 
O processo de treino do nosso modelo ocorre a cada 20 passos, ou quando chega a um estado final (final do episódio). Assim, ao não treinar em cada iteração, certificamo-nos que há dados suficientes para que o modelo treine de forma eficiente. Também por essa razão, os pesos só são atualizados a cada 200 passos, certificando assim que o modelo é treinado algumas vezes antes de atualizar os seus pesos.


Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5: Para avaliar a performance do agente neste exercício, apoiamo-nos em dois gráficos que mostram a evolução dos scores e steps por episódios. Além disso, o nosso programa produz, a cada step, uma representação do estado do tabuleiro em forma de imagem para permitir uma análise detalhada e intuitiva aos movimentos da cobra.
Tendo usado o algoritmo épsilon-greedy com decay neste exercício, a maior parte das ações nos primeiros episódios foram escolhidas aleatoriamente e portanto têm pouca relevância para esta análise.
Do episódio 500 para a frente, é possível observar uma gradual melhoria tanto nos scores como nos steps. No entanto, estes resultados estão ainda muito afastados do que procurávamos. Observando os movimentos da cobra passo-a-passo, pudemos observar que a cobra parece aprender a evitar as bordas do tabuleiro e perde embatendo contra si mesma. Além disso, os jogos acabam quase exclusivamente com um movimento à esquerda (ação -1). Uma grande percentagem dos jogos chegam até a acabar com 3 movimentos seguidos à esquerda. Esta informação é relevante e possivelmente indicativa do que pode estar mal na nossa abordagem, mas fomos incapazes de encontrar a causa deste defeito.
Embora estejamos razoavelmente satisfeitos com o facto da cobra evitar as bordas do mapa, tanto no aspeto de procurar a fruta como de evitar o resto do seu corpo, os resultados estão aquém do esperado. O aumento dos scores e steps até ao episódio 3000 faz-nos ponderar se este aumento continuaria se a aprendizagem continuasse com mais alguns milhares de episódios até atingir resultados aceitáveis mas o grande consumo de tempo destes testes impediu-nos de o fazer.
O problema relatado acima impediu também o teste de outras funcionalidades pois com três viragens seguidas em qualquer direção a cobra bate contra si mesma e o episódio acaba, o que limita o número de steps por episódio e a informação que podemos tirar daí relativamente ao efeito de outras experiências/mudanças na performance do programa.
snake_scores_discount0.6.png
snake_steps_discount0.6.png

